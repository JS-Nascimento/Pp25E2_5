{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-09-01T23:58:57.534658Z"
    }
   },
   "source": [
    "# [1] Importação da Base Local\n",
    "# - Carrega o dataset diretamente do arquivo local dataset.csv no diretório do projeto.\n",
    "\n",
    "import os, glob, warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, roc_auc_score,\n",
    "    silhouette_score,\n",
    "    roc_curve\n",
    ")\n",
    "from sklearn.metrics import pairwise_distances_argmin_min\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "sns.set(style='whitegrid')\n",
    "\n",
    "# Carregamento do dataset local\n",
    "data_file = os.path.join(os.getcwd(), 'dataset.csv') if os.path.exists(os.path.join(os.getcwd(), 'dataset.csv')) else 'dataset.csv'\n",
    "if not os.path.exists(data_file):\n",
    "    raise FileNotFoundError('Arquivo dataset.csv não encontrado no diretório do projeto.')\n",
    "print(f'Arquivo carregado: {os.path.basename(data_file)}')\n",
    "df = pd.read_csv(data_file)\n",
    "print('Formato inicial:', df.shape)\n",
    "\n",
    "# [2] Pré-processamento e Divisão Treino/Teste\n",
    "target_col = None\n",
    "if 'explicit' in df.columns:\n",
    "    target_col = 'explicit'\n",
    "elif 'track_genre' in df.columns:\n",
    "    # Caso \"explicit\" não exista, usar \"track_genre\" (multiclasse).\n",
    "    # Para manter AUC-ROC binária, transformamos em binário pegando a classe majoritária vs. demais.\n",
    "    top_class = df['track_genre'].value_counts().idxmax()\n",
    "    df['target_binary'] = (df['track_genre'] == top_class).astype(int)\n",
    "    target_col = 'target_binary'\n",
    "elif 'popularity' in df.columns:\n",
    "    # Fallback: binariza popularidade acima da mediana\n",
    "    median_pop = df['popularity'].median()\n",
    "    df['target_binary'] = (df['popularity'] >= median_pop).astype(int)\n",
    "    target_col = 'target_binary'\n",
    "else:\n",
    "    raise ValueError('Não foi possível identificar uma coluna alvo adequada (explicit/track_genre/popularity).')\n",
    "\n",
    "# Seleciona features numéricas e remove o alvo\n",
    "num_df = df.select_dtypes(include=[np.number]).copy()\n",
    "if target_col not in num_df.columns:\n",
    "    # Se o alvo não é numérico, garante que está no df geral e anexa\n",
    "    y = df[target_col].astype(int)\n",
    "else:\n",
    "    y = num_df[target_col].astype(int)\n",
    "    num_df = num_df.drop(columns=[target_col])\n",
    "\n",
    "# Remove colunas claramente identificadoras caso existam\n",
    "for col in ['id', 'track_id', 'song_id']:\n",
    "    if col in num_df.columns:\n",
    "        num_df = num_df.drop(columns=[col])\n",
    "\n",
    "# Trata faltantes\n",
    "num_df = num_df.fillna(num_df.median(numeric_only=True))\n",
    "\n",
    "# Split estratificado\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    num_df.values, y.values, test_size=0.2, random_state=SEED, stratify=y\n",
    ")\n",
    "print('Treino:', X_train.shape, 'Teste:', X_test.shape)\n",
    "\n",
    "# Padronização (necessária para SVM e K-Means)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# [3] Clusterização K-Médias (método do cotovelo e silhueta)\n",
    "# - Ajusta K-Means somente no conjunto de TREINO.\n",
    "k_values = list(range(2, 13))\n",
    "inertias = []\n",
    "silhouettes = []\n",
    "used_kmeans_on_train = False\n",
    "for k in k_values:\n",
    "    kmeans_tmp = KMeans(n_clusters=k, random_state=SEED, n_init=10)\n",
    "    kmeans_tmp.fit(X_train_scaled)\n",
    "    used_kmeans_on_train = True\n",
    "    inertias.append(kmeans_tmp.inertia_)\n",
    "    # Silhueta apenas se houver mais de 1 cluster e amostras suficientes\n",
    "    try:\n",
    "        sil = silhouette_score(X_train_scaled, kmeans_tmp.labels_)\n",
    "    except Exception:\n",
    "        sil = np.nan\n",
    "    silhouettes.append(sil)\n",
    "\n",
    "# Determinação do K ótimo\n",
    "# - Silhueta: escolhe K com maior valor de silhueta\n",
    "sil_vals = np.array(silhouettes, dtype=float)\n",
    "sil_safe = np.where(np.isnan(sil_vals), -np.inf, sil_vals)\n",
    "best_sil_idx = int(np.argmax(sil_safe))\n",
    "best_sil_k = k_values[best_sil_idx]\n",
    "# - Cotovelo: distância máxima ao segmento entre (k_min, inertia_minK) e (k_max, inertia_maxK)\n",
    "x = np.array(k_values, dtype=float)\n",
    "y = np.array(inertias, dtype=float)\n",
    "x1, y1 = x[0], y[0]\n",
    "x2, y2 = x[-1], y[-1]\n",
    "y_hat = y1 + (y2 - y1) * (x - x1) / (x2 - x1)\n",
    "dist = y_hat - y  # y está abaixo; maior distância sugere cotovelo\n",
    "elbow_k = int(x[int(np.argmax(dist))])\n",
    "# K final de referência: usar o de melhor silhueta\n",
    "optimal_k = best_sil_k\n",
    "print(f'K ótimo (silhueta): {best_sil_k}')\n",
    "print(f'K de cotovelo (heurística): {elbow_k}')\n",
    "print(f'K selecionado para referência: {optimal_k}')\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 4))\n",
    "ax[0].plot(k_values, inertias, marker='o')\n",
    "ax[0].axvline(elbow_k, color='green', linestyle=':', label='cotovelo')\n",
    "ax[0].set_title('Método do Cotovelo (Inertia)')\n",
    "ax[0].set_xlabel('K')\n",
    "ax[0].set_ylabel('Inertia')\n",
    "ax[0].legend()\n",
    "ax[1].plot(k_values, silhouettes, marker='o', color='orange')\n",
    "ax[1].axvline(best_sil_k, color='purple', linestyle=':', label='melhor silhueta')\n",
    "ax[1].set_title('Índice de Silhueta')\n",
    "ax[1].set_xlabel('K')\n",
    "ax[1].set_ylabel('Silhouette')\n",
    "ax[1].legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# [4] Criação da feature de distância ao centróide mais próximo\n",
    "# - Para cada K, ajusta K-Means no TREINO, calcula a distância mínima aos centróides\n",
    "#   e adiciona uma NOVA feature (1 coluna) a X_train/X_test.\n",
    "def build_distance_feature_for_k(Xtr_scaled, Xte_scaled, k):\n",
    "    km = KMeans(n_clusters=k, random_state=SEED, n_init=10)\n",
    "    km.fit(Xtr_scaled)\n",
    "    # Distâncias ao centróide mais próximo\n",
    "    _, dtr = pairwise_distances_argmin_min(Xtr_scaled, km.cluster_centers_)\n",
    "    _, dte = pairwise_distances_argmin_min(Xte_scaled, km.cluster_centers_)\n",
    "    Xtr_new = np.hstack([Xtr_scaled, dtr.reshape(-1, 1)])\n",
    "    Xte_new = np.hstack([Xte_scaled, dte.reshape(-1, 1)])\n",
    "    return Xtr_new, Xte_new\n",
    "\n",
    "# [5] Modelos SVM e Random Forest\n",
    "models = {\n",
    "    'svm_linear': SVC(kernel='linear', C=1.0, probability=True, random_state=SEED),\n",
    "    'svm_poly': SVC(kernel='poly', degree=3, C=1.0, gamma='scale', probability=True, random_state=SEED),\n",
    "    'svm_rbf': SVC(kernel='rbf', C=1.0, gamma='scale', probability=True, random_state=SEED),\n",
    "    'rf_base': RandomForestClassifier(n_estimators=300, max_depth=None, random_state=SEED),\n",
    "    'rf_depth10': RandomForestClassifier(n_estimators=400, max_depth=10, random_state=SEED),\n",
    "}\n",
    "\n",
    "# [6] Treinamento, Avaliação e Coleta de Métricas\n",
    "def eval_metrics(y_true, y_pred, y_proba):\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    prec = precision_score(y_true, y_pred, zero_division=0)\n",
    "    rec = recall_score(y_true, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "    try:\n",
    "        auc = roc_auc_score(y_true, y_proba)\n",
    "    except Exception:\n",
    "        auc = np.nan\n",
    "    return acc, prec, rec, f1, auc\n",
    "\n",
    "results = []\n",
    "\n",
    "# Avaliação baseline (sem feature de distância)\n",
    "for mname, model in models.items():\n",
    "    clf = model\n",
    "    clf.fit(X_train_scaled, y_train)\n",
    "    y_pred = clf.predict(X_test_scaled)\n",
    "    if hasattr(clf, 'predict_proba'):\n",
    "        y_proba = clf.predict_proba(X_test_scaled)[:, 1]\n",
    "    else:\n",
    "        # fallback improvável (não usado pois SVC está com probability=True)\n",
    "        y_proba = None\n",
    "    acc, prec, rec, f1, auc = eval_metrics(y_test, y_pred, y_proba)\n",
    "    results.append({\n",
    "        'model': mname, 'with_cluster_feature': False, 'k': None,\n",
    "        'accuracy': acc, 'precision': prec, 'recall': rec, 'f1': f1, 'auc': auc\n",
    "    })\n",
    "\n",
    "# Avaliação com feature de distância para diferentes K\n",
    "for k in k_values:\n",
    "    Xtr_k, Xte_k = build_distance_feature_for_k(X_train_scaled, X_test_scaled, k)\n",
    "    for mname, model in models.items():\n",
    "        clf = model\n",
    "        clf.fit(Xtr_k, y_train)\n",
    "        y_pred = clf.predict(Xte_k)\n",
    "        if hasattr(clf, 'predict_proba'):\n",
    "            y_proba = clf.predict_proba(Xte_k)[:, 1]\n",
    "        else:\n",
    "            y_proba = None\n",
    "        acc, prec, rec, f1, auc = eval_metrics(y_test, y_pred, y_proba)\n",
    "        results.append({\n",
    "            'model': mname, 'with_cluster_feature': True, 'k': k,\n",
    "            'accuracy': acc, 'precision': prec, 'recall': rec, 'f1': f1, 'auc': auc\n",
    "        })\n",
    "\n",
    "res_df = pd.DataFrame(results)\n",
    "print('Resumo de resultados (top 10 por F1):')\n",
    "display(res_df.sort_values(['with_cluster_feature','f1'], ascending=[True, False]).head(10))\n",
    "\n",
    "# [7] GridSearch para SVM (baseline e K=optimal_k)\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "# Dados expandidos com K ótimo\n",
    "Xtr_opt, Xte_opt = build_distance_feature_for_k(X_train_scaled, X_test_scaled, optimal_k)\n",
    "\n",
    "def run_gs_and_eval(base_name, est, grid, Xtr, Xte, ytr, yte, with_feat, k_val):\n",
    "    gs = GridSearchCV(est, grid, scoring='f1', cv=cv, n_jobs=None, refit=True)\n",
    "    gs.fit(Xtr, ytr)\n",
    "    best = gs.best_estimator_\n",
    "    y_pred = best.predict(Xte)\n",
    "    y_proba = best.predict_proba(Xte)[:,1] if hasattr(best, 'predict_proba') else None\n",
    "    acc, prec, rec, f1, auc = eval_metrics(yte, y_pred, y_proba)\n",
    "    results.append({\n",
    "        'model': base_name + '_gs', 'with_cluster_feature': with_feat, 'k': k_val,\n",
    "        'accuracy': acc, 'precision': prec, 'recall': rec, 'f1': f1, 'auc': auc,\n",
    "        'grid_search': True, 'best_params': str(gs.best_params_)\n",
    "    })\n",
    "    return gs\n",
    "\n",
    "# Grids por kernel\n",
    "grid_linear = { 'C': [0.1, 1, 10, 100] }\n",
    "grid_poly   = { 'C': [0.1, 1, 10], 'degree': [2, 3, 4], 'gamma': ['scale', 'auto'] }\n",
    "grid_rbf    = { 'C': [0.1, 1, 10], 'gamma': ['scale', 0.01, 0.1, 1] }\n",
    "\n",
    "# Baseline (sem feature de distância)\n",
    "run_gs_and_eval('svm_linear', SVC(kernel='linear', probability=True, random_state=SEED), grid_linear,\n",
    "                X_train_scaled, X_test_scaled, y_train, y_test, False, None)\n",
    "run_gs_and_eval('svm_poly', SVC(kernel='poly', probability=True, random_state=SEED), grid_poly,\n",
    "                X_train_scaled, X_test_scaled, y_train, y_test, False, None)\n",
    "run_gs_and_eval('svm_rbf', SVC(kernel='rbf', probability=True, random_state=SEED), grid_rbf,\n",
    "                X_train_scaled, X_test_scaled, y_train, y_test, False, None)\n",
    "\n",
    "# Expandido (com feature de distância usando K ótimo)\n",
    "run_gs_and_eval('svm_linear', SVC(kernel='linear', probability=True, random_state=SEED), grid_linear,\n",
    "                Xtr_opt, Xte_opt, y_train, y_test, True, optimal_k)\n",
    "run_gs_and_eval('svm_poly', SVC(kernel='poly', probability=True, random_state=SEED), grid_poly,\n",
    "                Xtr_opt, Xte_opt, y_train, y_test, True, optimal_k)\n",
    "run_gs_and_eval('svm_rbf', SVC(kernel='rbf', probability=True, random_state=SEED), grid_rbf,\n",
    "                Xtr_opt, Xte_opt, y_train, y_test, True, optimal_k)\n",
    "\n",
    "res_df = pd.DataFrame(results)\n",
    "print('Resultados com GridSearch (SVM) — resumo por F1:')\n",
    "display(res_df[res_df.get('grid_search', False) == True]\n",
    "        .sort_values(['with_cluster_feature','f1'], ascending=[True, False])\n",
    "        .groupby(['model','with_cluster_feature']).head(1))\n",
    "\n",
    "# [7] Análise Comparativa e Gráficos\n",
    "# [7.1] GridSearch para Random Forest (baseline e K=optimal_k)\n",
    "rf_grid = {\n",
    "    'n_estimators': [200, 400, 800],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['sqrt', 'log2', None]\n",
    "}\n",
    "\n",
    "def run_gs_rf_and_eval(base_name, grid, Xtr, Xte, ytr, yte, with_feat, k_val):\n",
    "    rf = RandomForestClassifier(random_state=SEED)\n",
    "    gs = GridSearchCV(rf, grid, scoring='f1', cv=cv, n_jobs=None, refit=True)\n",
    "    gs.fit(Xtr, ytr)\n",
    "    best = gs.best_estimator_\n",
    "    y_pred = best.predict(Xte)\n",
    "    y_proba = best.predict_proba(Xte)[:,1]\n",
    "    acc, prec, rec, f1, auc = eval_metrics(yte, y_pred, y_proba)\n",
    "    results.append({\n",
    "        'model': base_name + '_rf_gs', 'with_cluster_feature': with_feat, 'k': k_val,\n",
    "        'accuracy': acc, 'precision': prec, 'recall': rec, 'f1': f1, 'auc': auc,\n",
    "        'grid_search': True, 'best_params': str(gs.best_params_)\n",
    "    })\n",
    "    return gs\n",
    "\n",
    "# Baseline (Random Forest)\n",
    "run_gs_rf_and_eval('rf', rf_grid, X_train_scaled, X_test_scaled, y_train, y_test, False, None)\n",
    "# Expandido com K ótimo (Random Forest)\n",
    "run_gs_rf_and_eval('rf', rf_grid, Xtr_opt, Xte_opt, y_train, y_test, True, optimal_k)\n",
    "\n",
    "res_df = pd.DataFrame(results)\n",
    "print('Resultados com GridSearch (Random Forest) — resumo por F1:')\n",
    "display(res_df[(res_df.get('grid_search', False) == True) & (res_df['model'].str.contains('rf_gs'))]\n",
    "        .sort_values(['with_cluster_feature','f1'], ascending=[True, False])\n",
    "        .groupby(['model','with_cluster_feature']).head(1))\n",
    "\n",
    "# [7.2] Curvas ROC (baseline vs expandido com K ótimo)\n",
    "def plot_roc_for_pair(estimator_baseline, estimator_expanded, Xb, Xe, ytr, yte, title):\n",
    "    est_b = estimator_baseline\n",
    "    est_b.fit(Xb, ytr)\n",
    "    proba_b = est_b.predict_proba(X_test_scaled)[:,1] if Xb is X_train_scaled else est_b.predict_proba(Xe)[:,1]\n",
    "    fpr_b, tpr_b, _ = roc_curve(yte, proba_b)\n",
    "    auc_b = roc_auc_score(yte, proba_b)\n",
    "\n",
    "    est_e = estimator_expanded\n",
    "    est_e.fit(Xe, ytr)\n",
    "    proba_e = est_e.predict_proba(Xe)[:,1]\n",
    "    fpr_e, tpr_e, _ = roc_curve(yte, proba_e)\n",
    "    auc_e = roc_auc_score(yte, proba_e)\n",
    "\n",
    "    plt.figure(figsize=(5.5,4))\n",
    "    plt.plot(fpr_b, tpr_b, label=f'baseline (AUC={auc_b:.3f})')\n",
    "    plt.plot(fpr_e, tpr_e, label=f'expandido (AUC={auc_e:.3f})')\n",
    "    plt.plot([0,1],[0,1],'k--', alpha=0.5)\n",
    "    plt.xlabel('FPR')\n",
    "    plt.ylabel('TPR')\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    print(f'ROC AUC — {title}: baseline={auc_b:.3f}, expandido={auc_e:.3f}')\n",
    "\n",
    "# Preparar pares para SVM RBF e RandomForest\n",
    "svm_rbf_base = SVC(kernel='rbf', C=1.0, gamma='scale', probability=True, random_state=SEED)\n",
    "svm_rbf_exp  = SVC(kernel='rbf', C=1.0, gamma='scale', probability=True, random_state=SEED)\n",
    "plot_roc_for_pair(svm_rbf_base, svm_rbf_exp, X_train_scaled, Xte_opt, y_train, y_test, 'ROC — SVM RBF (baseline vs expandido)')\n",
    "\n",
    "rf_base = RandomForestClassifier(random_state=SEED)\n",
    "rf_exp  = RandomForestClassifier(random_state=SEED)\n",
    "plot_roc_for_pair(rf_base, rf_exp, X_train_scaled, Xte_opt, y_train, y_test, 'ROC — Random Forest (baseline vs expandido)')\n",
    "\n",
    "metrics_to_plot = ['accuracy', 'precision', 'recall', 'f1', 'auc']\n",
    "unique_models = list(models.keys())\n",
    "\n",
    "# Linhas por K com linha base (sem feature) para cada modelo\n",
    "for mname in unique_models:\n",
    "    base_row = res_df[(res_df['model'] == mname) & (~res_df['with_cluster_feature'])].iloc[0]\n",
    "    fig, axes = plt.subplots(1, len(metrics_to_plot), figsize=(4*len(metrics_to_plot), 3), sharex=True)\n",
    "    fig.suptitle(f'Comparativo por K — {mname}')\n",
    "    for j, metric in enumerate(metrics_to_plot):\n",
    "        ax = axes[j] if len(metrics_to_plot) > 1 else axes\n",
    "        dfk = res_df[(res_df['model'] == mname) & (res_df['with_cluster_feature'])].sort_values('k')\n",
    "        ax.plot(dfk['k'], dfk[metric], marker='o', label='com dist')\n",
    "        ax.axhline(base_row[metric], color='red', linestyle='--', label='baseline')\n",
    "        ax.set_title(metric.upper())\n",
    "        ax.set_xlabel('K')\n",
    "        ax.set_ylabel(metric)\n",
    "        if j == 0:\n",
    "            ax.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Melhor resultado por modelo e se houve ganho sobre baseline\n",
    "best_by_model = []\n",
    "for mname in unique_models:\n",
    "    base = res_df[(res_df['model'] == mname) & (~res_df['with_cluster_feature'])].iloc[0]\n",
    "    with_feat = res_df[(res_df['model'] == mname) & (res_df['with_cluster_feature'])]\n",
    "    if not with_feat.empty:\n",
    "        best_row = with_feat.sort_values('f1', ascending=False).iloc[0]\n",
    "        gain = {m: best_row[m] - base[m] for m in metrics_to_plot}\n",
    "        best_by_model.append({\n",
    "            'model': mname, 'best_k': int(best_row['k']), **{f'best_{m}': best_row[m] for m in metrics_to_plot},\n",
    "            **{f'gain_{m}': gain[m] for m in metrics_to_plot}\n",
    "        })\n",
    "best_df = pd.DataFrame(best_by_model)\n",
    "print('Melhor resultado por modelo (ordenado por ganho de F1):')\n",
    "display(best_df.sort_values('gain_f1', ascending=False))\n"
   ],
   "id": "bb34d0113fa1453a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo carregado: dataset.csv\n",
      "Formato inicial: (114000, 21)\n",
      "Treino: (91200, 15) Teste: (22800, 15)\n"
     ]
    }
   ],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
